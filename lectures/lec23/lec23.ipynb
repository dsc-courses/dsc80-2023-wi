{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "TEMPLATE = 'seaborn'\n",
    "\n",
    "# Carryover setup from last lecture\n",
    "import seaborn as sns\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import util\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 23 ‚Äì Cross-Validation\n",
    "\n",
    "## DSC 80, Winter 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Project 4 is due **tomorrow at 11:59PM**.\n",
    "- Lab 9 (pipelines) is due on **Monday, March 13th at 11:59PM**.\n",
    "- Project 5 (prediction) is due on **Thursday, March 23rd at 11:59PM (no slip days allowed)**!\n",
    "- Project 3 scores have been released and the grade report has been updated. Check them both out on Gradescope.\n",
    "- Office hours from 6-9PM today and 12:30-6PM tomorrow are being held in the SDSC Auditorium ‚Äì come by to work on projects/labs! See the [calendar](https://dsc80.com/calendar) for more details.\n",
    "- [practice.dsc80.com](https://practice.dsc80.com) now contains 3 past finals. Start reviewing!\n",
    "    - Prioritize the Spring 2022 final.\n",
    "- RSVP to the Senior Capstone Showcase on March 15th at [**hdsishowcase.com**](https://hdsishowcase.com).\n",
    "    - There is no live lecture for DSC 80 on the day of the showcase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Generalization.\n",
    "- Train-test split.\n",
    "- Hyperparameters.\n",
    "- Cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall, last time, we drew two samples from the same data generating process, and fit polynomials of degree 1, 3, and 25 on each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(23) # For reproducibility.\n",
    "\n",
    "def sample_dgp(n=100):\n",
    "    x = np.linspace(-2, 3, n)\n",
    "    y = x ** 3 + (np.random.normal(0, 3, size=n))\n",
    "    return pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "sample_1 = sample_dgp()\n",
    "sample_2 = sample_dgp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When trained on sample 1, the degree 25 polynomial had the lowest RMSE on sample 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Look at the definition of train_and_plot in util.py if you're curious as to how the plotting works.\n",
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_1, degs=[1, 3, 25])\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But, when trained on sample 1, the degree 3 polynomial had the lowest RMSE on sample 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25])\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we train polynomials of degree 1, 3, and 25 on each sample, we see that the degree 25 polynomials vary more than the degree 1 and 3 polynomials do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.plot_multiple_models(sample_1, sample_2, degs=[1, 3, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias and variance\n",
    "\n",
    "The training data we have access to is a sample from the DGP. We are concerned with our model's ability to **generalize** and work well on **different datasets** drawn from the same DGP.\n",
    "\n",
    "Suppose we **fit** a model $H$ (e.g. a degree 3 polynomial) on **several different datasets** from a DGP. There are three sources of error that arise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* ‚≠êÔ∏è **Bias**: **The expected deviation between a predicted value and an actual value**.\n",
    "    - In other words, **for a given $x_i$, how far is $H(x_i)$ from the true $y_i$, on average?**\n",
    "    - Low bias is good! ‚úÖ\n",
    "    - High bias is a sign of **underfitting**, i.e. that our model is too **basic** to capture the relationship between our features and response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- ‚≠êÔ∏è **Model variance (\"variance\")**: **The variance of a model's predictions**.\n",
    "    - In other words, **for a given $x_i$, what is the variance of $H(x_i)$ across all datasets**?\n",
    "    - Low model variance is good! ‚úÖ\n",
    "    - High model variance is a sign of **overfitting**, i.e. that our model is too **complicated** and is prone to fitting to the noise in our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Observation variance**: The variance due to the random noise in the process we are trying to model (e.g. measurement error). _We can't control this, without collecting more data!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, suppose:\n",
    "- The <span style='color:#c6283f'><b>red bulls-eye</b></span> represents your **true weight and height** üßç.\n",
    "- The <span style='color:#080c6f'><b>dark blue darts</b></span> represent **predictions of your weight and height** using different models that were fit on the same DGP. \n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/image_5.png\" width=\"40%\"></center>\n",
    "\n",
    "We'd like our models to be in the top left, but in practice that's hard to achieve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Avoiding overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We won't know whether our model has **overfit** to our sample (training data) unless we get to see how well it performs on a new sample from the same DGP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- üí°**Idea**: **Split** our sample into a **training set** and **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use **only** the training set to fit the model (i.e. find $w^*$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Use the test set to evaluate the model's error (RMSE, $R^2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The test set is like a new sample of data from the same DGP as the training data!\n",
    "    - _Similar_ to bootstrapping (but not quite the same, because there is no resampling involved).\n",
    "    - **If our sample is not representative of the DGP, this method has limited effectiveness!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"imgs/train-test.png\" width='50%'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train-test split üöÜ\n",
    "\n",
    "`sklearn.model_selection.train_test_split` implements a train-test split for us! üôèüèº "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If `X` is an array/DataFrame of features and `y` is an array/Series of responses,\n",
    "\n",
    "```py\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "```\n",
    "\n",
    "randomly splits the features and responses into training and test sets, such that the test set contains 0.25 of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the documentation!\n",
    "train_test_split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's perform a train/test split on our `tips` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # We don't have to choose 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Before proceeding, let's check the sizes of `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Rows in X_train:', X_train.shape[0])\n",
    "display(X_train.head())\n",
    "print('Rows in X_test:', X_test.shape[0])\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.shape[0] / tips.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example train-test split\n",
    "\n",
    "Steps:\n",
    "1. Fit a model on the training set.\n",
    "2. Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips[['total_bill', 'size']] # For this example, we'll use just the already-quantitative columns in tips.\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # random_state is like np.random.seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, we'll use a stand-alone `LinearRegression` model without a `Pipeline`, but this process would work the same if we were using a `Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's check our model's performance on the **training** set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error # Built-in RMSE/MSE function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = lr.predict(X_train)\n",
    "rmse_train = mean_squared_error(y_train, pred_train, squared=False)\n",
    "rmse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And the **test** set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = lr.predict(X_test)\n",
    "rmse_test = mean_squared_error(y_test, pred_test, squared=False)\n",
    "rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since `rmse_train` and `rmse_test` are similar, it **doesn't seem like our model is overfitting** to the training data. If `rmse_test` was much larger than `rmse_train`, it would be evidence that our model is unable to **generalize well**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Polynomial regression\n",
    "\n",
    "We recently looked at an example of **polynomial regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = util.train_and_plot(train_sample=sample_1, test_sample=sample_2, degs=[1, 3, 25])\n",
    "fig.update_layout(title='Trained on Sample 1, Performance on Sample 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building these models:\n",
    "- We **got to choose** the degree of the polynomials (i.e. we chose 1, 3, and 25).\n",
    "- We didn't get to choose the exact formulas for the three polynomials ‚Äì their formulas were **learned from data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Parameters vs. hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **parameter** defines the relationship between variables in a model. \n",
    "    - **We learn parameters from data**.\n",
    "    - For instance, suppose we fit a degree 3 polynomial to data, and end up with\n",
    "    \n",
    "    $$H(x) = 1 - 2x + 13x^2 - 4x^3$$\n",
    "    \n",
    "    - 1, -2, 13, and -4 are parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A **hyperparameter** is a parameter that we get to choose **before our model is fit to the data**.\n",
    "    - Think of hyperparameters as knobs üéõ ‚Äì **we get to pick and tune them!**\n",
    "    - **Polynomial degree** was a hyperparameter in the previous example, and we tried three different values ‚Äì 1, 3, and 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question:** How do we choose the \"right\" hyperparameter(s)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We know that a model's performance on a **test set** is a good estimate of its ability to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We want to find the hyperparameter that leads to the best **test set performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Idea:\n",
    "    1. Come up with a **list** of hyperparameters to try.\n",
    "    2. For each hyperparameter, train the model on the training set and compute its performance on the test set.\n",
    "    3. Pick the hyperparameter with the best performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's try this strategy on sample 1 from our earlier example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll try to fit a polynomial model on the dataset; we'll choose the polynomial's degree from the list [1, 2, ..., 25]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1', template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we perform a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample_1[['x']]\n",
    "y = sample_1['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll implement the logic from the previous slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errs = []\n",
    "test_errs = []\n",
    "\n",
    "for d in range(1, 26):\n",
    "    pl = Pipeline([('poly', PolynomialFeatures(d)), ('lin-reg', LinearRegression())])\n",
    "    pl.fit(X_train, y_train)\n",
    "    train_errs.append(mean_squared_error(y_train, pl.predict(X_train), squared=False))\n",
    "    test_errs.append(mean_squared_error(y_test, pl.predict(X_test), squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the plots of training error vs. degree and test error vs. degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(1, 26), y=train_errs, name='Training Error')\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=np.arange(1, 26), y=test_errs, name='Test Error', line={'color': 'orange'})\n",
    ")\n",
    "\n",
    "fig.update_layout(showlegend=True, xaxis_title='Degree', yaxis_title='RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Training error appears to decrease as polynomial degree increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Test error appears to decrease until a \"valley\", and then increases again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Here, we'd choose a degree of 3, since that degree has the **lowest test error**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. test error\n",
    "\n",
    "The pattern we saw in the previous example is true more generally.\n",
    "\n",
    "<center><img src='imgs/tt-errors.png' width=50%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the hyperparameter(s) at the \"valley\" of test error.\n",
    "\n",
    "Note that training error **tends** to underestimate test error, but it doesn't have to ‚Äì i.e., it is possible for test error to be lower than training error (say, if the test set is \"easier\" to predict than the training set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conducting train-test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Recall, <span style='color: blue'><b>training data</b></span> is used to fit our model, and <span style='color: orange'><b>test data</b></span> is used to evaluate our model.\n",
    "\n",
    "<center><img src='imgs/train-test-first.png' width=40%></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question:** _How_ should we split?\n",
    "    - `sklearn`'s `train_test_split` splits **randomly**, which usually works well.\n",
    "    - However, if there is some element of **time** in the training data (say, when predicting the future price of a stock), a better split is \"past\" and \"future\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question:** How _large_ should the split be, e.g. 90%-10% vs. 75%-25%?\n",
    "    - There's a tradeoff ‚Äì a larger training set should lead to a \"better\" model, while a larger test set should lead to a better estimate of our model's ability to generalize.\n",
    "    - There's no \"right\" choice, but we usually choose between a split between the ranges above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### But wait..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With our current strategy, we are choosing the hyperparameter that creates the model that **performs best on the test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- As such, we are **overfitting to the test set** ‚Äì the best hyperparameter for the test set might not be the best hyperparameter for a totally unseen dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It seems like we need **another** split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A single validation set\n",
    "\n",
    "<center><img src='imgs/train-test-val.png' width=40%></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Split the data into three sets: <span style='color: blue'><b>training</b></span>, <span style='color: green'><b>validation</b></span>, and <span style='color: orange'><b>test</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. For each hyperparameter choice, <span style='color: blue'><b>train</b></span> the model only on the <span style='color: blue'><b>training set</b></span>, and <span style='color: green'><b>evaluate</b></span> the model's performance on the <span style='color: green'><b>validation set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "3. Find the hyperparameter with the best <span style='color: green'><b>validation</b></span> performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "4. Retrain the final model on the <span style='color: blue'><b>training</b></span> and <span style='color: green'><b>validation</b></span> sets, and report its performance on the <span style='color: orange'><b>test set</b></span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Issue:** This strategy is too dependent on the <span style='color: green'><b>validation</b></span> set, which may be small and/or not a representative sample of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation\n",
    "\n",
    "Instead of relying on a single validation set, we can create $k$ validation sets, where $k$ is some positive integer (5 in the example below).\n",
    "\n",
    "<center><img src='imgs/k-fold.png' width=40%></center>\n",
    "\n",
    "Since each data point is used for training $k-1$ times and validation once, the (averaged) validation performance should be a good metric of a model's ability to generalize to unseen data.\n",
    "\n",
    "$k$-fold cross-validation (or simply \"cross-validation\") is **the** technique we will use for finding hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating folds in `sklearn`\n",
    "\n",
    "`sklearn` has a `KFold` class that splits data into training and validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple dataset for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.arange(10, 70, 10)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a `KFold` object with $k=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(3, shuffle=True, random_state=1)\n",
    "kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's use `kfold` to `split` `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train, val in kfold.split(data):\n",
    "    print(f'train: {data[train]}, validation: {data[val]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each value in `data` is used for validation exactly once and for training exactly twice. Also note that because we set `shuffle=True` the groups are not simply `[10, 20]`, `[30, 40]`, and `[50, 60]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation\n",
    "\n",
    "First, **shuffle** the dataset randomly and **split** it into $k$ disjoint groups. Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- For each hyperparameter:\n",
    "    - For each unique group:\n",
    "        - Let the unique group be the \"validation set\".\n",
    "        - Let all other groups be the \"training set\".\n",
    "        - Train a model using the selected hyperparameter on the training set.\n",
    "        - Evaluate the model on the validation set.\n",
    "    - Compute the **average** validation score (e.g. RMSE) for the particular hyperparameter.\n",
    "- Choose the hyperparameter with the best average validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation in `sklearn`\n",
    "\n",
    "While you could manually use `KFold` to perform cross-validation, the `cross_val_score` function in `sklearn` implements $k$-fold cross-validation for us! \n",
    "\n",
    "```py\n",
    "cross_val_score(estimator, X_train, y_train, cv)\n",
    "```\n",
    "\n",
    "Specifically, it takes in:\n",
    "- A `Pipeline` or estimator **that has not already been `fit`**.\n",
    "- Training data.\n",
    "- A value of $k$ (through the `cv` argument).\n",
    "- (Optionally) A `scoring` metric.\n",
    "\n",
    "and performs $k$-fold cross-validation, returning the values of the scoring metric on each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's perform $k$-fold cross validation in order to help us pick a degree for polynomial regression from the list [1, 2, ..., 25]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll use $k=5$ since it's a common choice (and the default in `sklearn`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For the sake of this example, we'll suppose `sample_1` is our \"training + validation data\", i.e. that our test data is in some other dataset.\n",
    "    - If this were not true, we'd first need to split `sample_1` into separate training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df = pd.DataFrame()\n",
    "\n",
    "for d in range(1, 26):\n",
    "    pl = Pipeline([('poly', PolynomialFeatures(d)), ('lin-reg', LinearRegression())])\n",
    "    \n",
    "    # The `scoring` argument is used to specify that we want to compute the RMSE; \n",
    "    # the default is R^2. It's called \"neg\" RMSE because, \n",
    "    # by default, sklearn likes to \"maximize\" scores, and maximizing -RMSE is the same\n",
    "    # as minimizing RMSE.\n",
    "    errs = cross_val_score(pl, sample_1[['x']], sample_1['y'], \n",
    "                           cv=5, scoring='neg_root_mean_squared_error')\n",
    "    errs_df[f'Deg {d}'] = -errs # Negate to turn positive (sklearn computed negative RMSE).\n",
    "    \n",
    "errs_df.index = [f'Fold {i}' for i in range(1, 6)]\n",
    "errs_df.index.name = 'Validation Fold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Next class, we'll look at how to implement this procedure without needing to `for`-loop over values of `d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation in `sklearn`\n",
    "\n",
    "Note that for each choice of degree (our hyperparameter), we have **five** RMSEs, one for each \"fold\" of the data. This means that in total, 125 models were trained/fit to data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We should choose the degree with the lowest **average** validation RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df.mean().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that if we didn't perform $k$-fold cross-validation, but instead just used a single validation set, we may have ended up with a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df.idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***Note***: You may notice that the RMSEs in Folds 1 and 5 are significantly higher than in other folds. Can you think of reasons why, and how we might fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "px.scatter(sample_1, x='x', y='y', title='Sample 1', template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Tips\n",
    "\n",
    "We can also use $k$-fold cross-validation to determine which subset of features to use in a linear model that predicts tips (though, as you'll see, the code is not pretty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we should always do, we'll perform a train-test split on `tips` and will only use the training data for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary that maps names to Pipeline objects.\n",
    "pipes = {\n",
    "    'total_bill only': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "    'total_bill + size': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill', 'size'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "    'total_bill + size + OHE smoker': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill', 'size']),\n",
    "             ('ohe', OneHotEncoder(), ['smoker'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "    'total_bill + size + OHE all': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill', 'size']),\n",
    "             ('ohe', OneHotEncoder(), ['smoker', 'sex', 'time', 'day'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df = pd.DataFrame()\n",
    "\n",
    "for pipe in pipes:\n",
    "    errs = cross_val_score(pipes[pipe], X_train, y_train,\n",
    "                           cv=5, scoring='neg_root_mean_squared_error')\n",
    "    pipe_df[pipe] = -errs\n",
    "    \n",
    "pipe_df.index = [f'Fold {i}' for i in range(1, 6)]\n",
    "pipe_df.index.name = 'Validation Fold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df.mean().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the third model has the lowest average validation RMSE, its average validation RMSE is very close to that of the other, simpler models, and as a result we'd likely use the simplest model in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Generalization\n",
    "\n",
    "1. Split the data into two sets: <span style='color: blue'><b>training</b></span> and <span style='color: orange'><b>test</b></span>.\n",
    "\n",
    "2. Use only the <span style='color: blue'><b>training</b></span> data when designing, training, and tuning the model.\n",
    "    - Use <span style='color: green'><b>$k$-fold cross-validation</b></span> to choose hyperparameters and estimate the model's ability to generalize.\n",
    "    - Do not ‚ùå look at the <span style='color: orange'><b>test</b></span> data in this step!\n",
    "    \n",
    "3. Commit to your final model and train it using the entire <span style='color: blue'><b>training</b></span> set.\n",
    "\n",
    "4. Test the data using the <span style='color: orange'><b>test</b></span> data. If the performance (e.g. RMSE) is not acceptable, return to step 2.\n",
    "\n",
    "5. Finally, train on **all available data** and ship the model to production! üõ≥\n",
    "\n",
    "üö® This is the process you should **always** use! üö® "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion Question ü§î\n",
    "\n",
    "- Suppose you have a training dataset with 1000 rows.\n",
    "- You want to decide between 20 hyperparameters for a particular model.\n",
    "- To do so, you perform 10-fold cross-validation.\n",
    "- **How many times is the first row in the training dataset (`X.iloc[0]`) used for training a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- A model's training error tends to decrease as model complexity increases, while its test error tends to decrease, before reaching a \"sweet spot\" and increasing again.\n",
    "- A hyperparameter is a configuration that we choose before training a model; an important task in machine learning is selecting \"good\" hyperparameters.\n",
    "- In order to quantify a model's ability to generalize to unseen data, use **$k$-fold cross-validation**.\n",
    "    - In particular, $k$-fold CV is used to select hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Next time\n",
    "\n",
    "- Example: Decision trees üå≤.\n",
    "- Tuning multiple hyperparameters at once.\n",
    "- Multicollinearity (time permitting)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
