{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "TEMPLATE = 'seaborn'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 24 ‚Äì Decision Trees, Grid Search, Multicollinearity\n",
    "\n",
    "## DSC 80, Winter 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Lab 9 (pipelines) is due on Monday, March 13th at 11:59PM.\n",
    "- Project 5 (prediction) is due on Thursday, March 23rd at 11:59PM (no slip days allowed)!\n",
    "- [practice.dsc80.com](https://practice.dsc80.com) now contains 3 past finals. Start reviewing!\n",
    "    - Prioritize the Spring 2022 final.\n",
    "- There is no live lecture next Wednesday or Friday; videos will be posted instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Cross-validation.\n",
    "- Example: Decision trees üå≤.\n",
    "- Grid search.\n",
    "- Multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Suppose we've decided to fit a polynomial regressor on a dataset $\\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\}$, but we're unsure of what degree of polynomial to use (1, 2, 3, ..., 25). \n",
    "    - Note that polynomial degree is a **hyperparameter** ‚Äì it is something we can control _before_ our model is fit to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Or, suppose we have a dataset of restaurant tips, bills, table sizes, days of the week, and so on, and want to decide which features to use in a linear model that predicts tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember, more complicated models (that is, models with more features) don't necessarily **generalize** well to **unseen data**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Goal**: Find the best hyperparameter, or best choice of features, so that our fit model **generalizes well to unseen data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation\n",
    "\n",
    "Instead of relying on a single validation set, we can create $k$ validation sets, where $k$ is some positive integer (5 in the example below).\n",
    "\n",
    "<center><img src='imgs/k-fold.png' width=40%></center>\n",
    "\n",
    "Since each data point is used for training $k-1$ times and validation once, the (averaged) validation performance should be a good metric of a model's ability to generalize to unseen data.\n",
    "\n",
    "$k$-fold cross-validation (or simply \"cross-validation\") is **the** technique we will use for finding hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation\n",
    "\n",
    "First, **shuffle** the dataset randomly and **split** it into $k$ disjoint groups. Then:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- For each hyperparameter:\n",
    "    - For each unique group:\n",
    "        - Let the unique group be the \"validation set\".\n",
    "        - Let all other groups be the \"training set\".\n",
    "        - Train a model using the selected hyperparameter on the training set.\n",
    "        - Evaluate the model on the validation set.\n",
    "    - Compute the **average** validation score (e.g. RMSE) for the particular hyperparameter.\n",
    "- Choose the hyperparameter with the best average validation score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As a reminder, here's what \"sample 1\" looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_1 = pd.read_csv(os.path.join('data', 'sample-1.csv'))\n",
    "px.scatter(x=sample_1['x'], y=sample_1['y'], template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's perform $k$-fold cross validation in order to help us pick a degree for polynomial regression from the list [1, 2, ..., 25]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We'll use $k=5$ since it's a common choice (and the default in `sklearn`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- For the sake of this example, we'll suppose `sample_1` is our \"training + validation data\", i.e. that our test data is in some other dataset.\n",
    "    - If this were not true, we'd first need to split `sample_1` into separate training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df = pd.DataFrame()\n",
    "\n",
    "for d in range(1, 26):\n",
    "    pl = Pipeline([('poly', PolynomialFeatures(d)), ('lin-reg', LinearRegression())])\n",
    "    \n",
    "    # The `scoring` argument is used to specify that we want to compute the RMSE; \n",
    "    # the default is R^2. It's called \"neg\" RMSE because, \n",
    "    # by default, sklearn likes to \"maximize\" scores, and maximizing -RMSE is the same\n",
    "    # as minimizing RMSE.\n",
    "    errs = cross_val_score(pl, sample_1[['x']], sample_1['y'], \n",
    "                           cv=5, scoring='neg_root_mean_squared_error')\n",
    "    errs_df[f'Deg {d}'] = -errs # Negate to turn positive (sklearn computed negative RMSE).\n",
    "    \n",
    "errs_df.index = [f'Fold {i}' for i in range(1, 6)]\n",
    "errs_df.index.name = 'Validation Fold'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Soon, we'll look at how to implement this procedure without needing to `for`-loop over values of `d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $k$-fold cross-validation in `sklearn`\n",
    "\n",
    "Note that for each choice of degree (our hyperparameter), we have **five** RMSEs, one for each \"fold\" of the data. This means that in total, 125 models were trained/fit to data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We should choose the degree with the lowest **average** validation RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df.mean().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that if we didn't perform $k$-fold cross-validation, but instead just used a single validation set, we may have ended up with a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_df.idxmin(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "***Note***: You may notice that the RMSEs in Folds 1 and 5 are significantly higher than in other folds. Can you think of reasons why, and how we might fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Another example: Tips\n",
    "\n",
    "We can also use $k$-fold cross-validation to determine which subset of features to use in a linear model that predicts tips (though, as you'll see, the code is not pretty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "tips = sns.load_dataset('tips')\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we should always do, we'll perform a train-test split on `tips` and will only use the training data for cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tips.drop('tip', axis=1)\n",
    "y = tips['tip']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary that maps names to Pipeline objects.\n",
    "pipes = {\n",
    "    'total_bill only': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "    'total_bill + size': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill', 'size'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "    'total_bill + size + OHE smoker': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill', 'size']),\n",
    "             ('ohe', OneHotEncoder(), ['smoker'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "    'total_bill + size + OHE all': Pipeline([\n",
    "        ('trans', ColumnTransformer(\n",
    "            [('keep', FunctionTransformer(lambda x: x), ['total_bill', 'size']),\n",
    "             ('ohe', OneHotEncoder(), ['smoker', 'sex', 'time', 'day'])], \n",
    "            remainder='drop')), \n",
    "        ('lin-reg', LinearRegression())\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df = pd.DataFrame()\n",
    "\n",
    "for pipe in pipes:\n",
    "    errs = cross_val_score(pipes[pipe], X_train, y_train,\n",
    "                           cv=5, scoring='neg_root_mean_squared_error')\n",
    "    pipe_df[pipe] = -errs\n",
    "    \n",
    "pipe_df.index = [f'Fold {i}' for i in range(1, 6)]\n",
    "pipe_df.index.name = 'Validation Fold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_df.mean().idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the third model has the lowest average validation RMSE, its average validation RMSE is very close to that of the other, simpler models, and as a result we'd likely use the simplest model in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary: Generalization\n",
    "\n",
    "1. Split the data into two sets: <span style='color: blue'><b>training</b></span> and <span style='color: orange'><b>test</b></span>.\n",
    "\n",
    "2. Use only the <span style='color: blue'><b>training</b></span> data when designing, training, and tuning the model.\n",
    "    - Use <span style='color: green'><b>$k$-fold cross-validation</b></span> to choose hyperparameters and estimate the model's ability to generalize.\n",
    "    - Do not ‚ùå look at the <span style='color: orange'><b>test</b></span> data in this step!\n",
    "    \n",
    "3. Commit to your final model and train it using the entire <span style='color: blue'><b>training</b></span> set.\n",
    "\n",
    "4. Test the data using the <span style='color: orange'><b>test</b></span> data. If the performance (e.g. RMSE) is not acceptable, return to step 2.\n",
    "\n",
    "5. Finally, train on **all available data** and ship the model to production! üõ≥\n",
    "\n",
    "üö® This is the process you should **always** use! üö® "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Discussion Question ü§î\n",
    "\n",
    "- Suppose you have a training dataset with 1000 rows.\n",
    "- You want to decide between 20 hyperparameters for a particular model.\n",
    "- To do so, you perform 10-fold cross-validation.\n",
    "- **How many times is the first row in the training dataset (`X.iloc[0]`) used for training a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Decision trees üå≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src='imgs/taxonomy.png' width=50%></center>\n",
    "\n",
    "Decision trees can be used for both regression and classification. We will start by discussing their use in **classification**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Predicting diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0 means no diabetes, 1 means yes diabetes.\n",
    "diabetes['Outcome'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'Glucose'` is measured in mg/dL (milligrams per deciliter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `'BMI'` is calculated as $\\text{BMI} = \\frac{\\text{weight (kg)}}{\\left[ \\text{height (m)} \\right]^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's use `'Glucose'` and `'BMI'` to predict whether or not a patient has diabetes (`'Outcome'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploring the dataset\n",
    "\n",
    "<span style='color: orange'><b>Class 0 (orange) is \"no diabetes\"</b></span> and <span style='color: blue'><b>class 1 (blue) is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = (\n",
    "    diabetes.assign(Outcome=diabetes['Outcome'].astype(str))\n",
    "            .plot(kind='scatter', x='Glucose', y='BMI', color='Outcome', \n",
    "                  color_discrete_map={'0': 'orange', '1': 'blue'},\n",
    "                  title='Relationship between Glucose, BMI, and Diabetes',\n",
    "                  template=TEMPLATE)\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's build a decision tree and interpret the results. But first, a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes[['Glucose', 'BMI']], \n",
    "                                                    diabetes['Outcome'],\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relevant class is `DecisionTreeClassifier`, from `sklearn.tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we `fit` it the same way we `fit` earlier estimators.\n",
    "\n",
    "_You may wonder what `max_depth=2` does ‚Äì more on this soon!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualizing decision trees\n",
    "\n",
    "Our fit decision tree is like a \"flowchart\", made up of a series of questions.\n",
    "\n",
    "As before, <span style='color: orange'><b>orange is \"no diabetes\"</b></span> and <span style='color: blue'><b>blue  is \"diabetes\"</b></span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plot_tree(dt, feature_names=X_train.columns, class_names=['no db', 'yes db'], \n",
    "          filled=True, rounded=True, fontsize=15, impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To **classify a new data point**, we start at the top and answer the first question (i.e. \"Glucose <= 129.5\").\n",
    "- If the answer is \"**Yes**\", we move to the **left** branch, otherwise we move to the right branch.\n",
    "- We repeat this process until we end up at a leaf node, at which point we predict the most common class in that node.\n",
    "    - Note that each node has a `value` attribute, which describes the number of **training** individuals of each class that fell in that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the left node at depth 2 has a `value` of [304, 78].\n",
    "y_train.loc[X_train[X_train['Glucose'] <= 129.5].index].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluating classifiers\n",
    "\n",
    "The most common evaluation metric in classification is **accuracy**:\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{# data points classified correctly}}{\\text{# data points}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dt.predict(X_train) == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The `score` method of a classifier computes accuracy by default (just like the `score` method of a regressor computes $R^2$ by default). We want our classifiers to have **high accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy ‚Äì same number as above\n",
    "dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Some questions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How did `sklearn` **learn** what questions to ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Can we get it to ask more questions (i.e. build a **deeper** tree)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training a decision tree\n",
    "\n",
    "When we ask a question, we are effectively **splitting** a node into two children ‚Äì the \"yes\" child and the \"no\" child."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Suppose the distribution within a node looks like this (colors represent classes):\n",
    "\n",
    "<center>üü†üü†üü†üîµüîµüîµüîµüîµüîµüîµ</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question A **splits** the node like this:\n",
    "- \"Yes\": üü†üü†üîµüîµüîµ\n",
    "- \"No\": üü†üîµüîµüîµüîµ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question B **splits** the node like this:\n",
    "- \"Yes\": üîµüîµüîµüîµüîµüîµ\n",
    "- \"No\": üîµüü†üü†üü†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Which question is \"better\"?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Question B, because there is \"less uncertainty\" in the resulting nodes after splitting by Question B than there is after splitting by Question A. There are two common techniques for quantifying \"uncertainty\":\n",
    "- Gini impurity (the default in `sklearn`).\n",
    "- Entropy (from information theory).\n",
    "\n",
    "Not the focus of our course, but read more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tree depth\n",
    "\n",
    "Decision trees are trained by **recursively** picking the best split until:\n",
    "- all \"leaf nodes\" only contain training examples from a single class (good), or\n",
    "- it is impossible to split leaf nodes any further (not good)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By default, there is no \"maximum depth\" for a decision tree. As such, without restriction, decision trees tend to be very deep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_no_max = DecisionTreeClassifier()\n",
    "dt_no_max.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree fit on our training data has a depth of around 20! (It is so deep that `tree.plot_tree` errors when trying to plot it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_no_max.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "At first, this tree seems \"better\" than our tree of depth 2, since its training accuracy is much much higher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_no_max.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth 2 tree.\n",
    "dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But recall, we truly care about **test set performance**, and this decision tree has **worse accuracy on the test set than our depth 2 tree**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_no_max.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth 2 tree.\n",
    "dt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision trees and overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Decision trees have a tendency to overfit. **Why is that?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Unlike linear classification techniques (like logistic regression or SVMs), **decision trees are non-linear**.\n",
    "    - They are also \"non-parametric\" ‚Äì there are no $w^*$s to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- While being trained, decision trees ask enough questions to effectively **memorize** the correct response values in the training set. However, the relationships they learn are often overfit to the noise in the training set, and don't generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A decision tree whose depth is not restricted will achieve 100% accuracy on any training set, as long as there are no \"overlapping values\" in the training set.\n",
    "    - Two values overlap when they have the same features $x$ but different response values $y$ (e.g. if two patients have the same glucose levels and BMI, but one has diabetes and one doesn't)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **One solution**: Make the decision tree \"less complex\" by limiting the maximum depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since `sklearn.tree`'s `plot_tree` can't visualize extremely large decision trees, let's create and visualize some smaller decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees = {}\n",
    "for d in [2, 4, 8]:\n",
    "    trees[d] = DecisionTreeClassifier(max_depth=d, random_state=1)\n",
    "    trees[d].fit(X_train, y_train)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5), dpi=100)\n",
    "    plot_tree(trees[d], feature_names=X_train.columns, class_names=['no db', 'yes db'], \n",
    "               filled=True, rounded=True, impurity=False)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As tree depth increases, complexity increases, and our trees are more prone to overfitting.\n",
    "\n",
    "**Question**: What is the \"right\" maximum depth to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hyperparameters for decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `max_depth` is a hyperparameter for `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are many more hyperparameters we can tweak; look at [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for examples.\n",
    "    - `min_samples_split`: The minimum number of samples required to split an internal node.\n",
    "    - `criterion`: The function to measure the quality of a split (`'gini'` or `'entropy'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- To ensure that our model generalizes well to unseen data, we need an efficient technique for trying different combinations of hyperparameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid search\n",
    "\n",
    "`GridSearchCV` takes in:\n",
    "- an **un-`fit`** instance of an estimator, and\n",
    "- a **dictionary** of hyperparameter values to try,\n",
    "\n",
    "and performs $k$-fold cross-validation to find the **combination of hyperparameters with the best average validation performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following dictionary contains the values we're considering for each hyperparameter. (We're using `GridSearchCV` with 3 hyperparameters, but we could use it with even just a single hyperparameter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'max_depth': [2, 3, 4, 5, 7, 10, 13, 15, 18, None], \n",
    "    'min_samples_split': [2, 5, 10, 20, 50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that there are 140 **combinations** of hyperparameters we need to try. We need to find the **best combination** of hyperparameters, not the best value for each hyperparameter individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hyperparameters['max_depth']) * \\\n",
    "len(hyperparameters['min_samples_split']) * \\\n",
    "len(hyperparameters['criterion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`GridSearchCV` needs to be instantiated and `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = GridSearchCV(DecisionTreeClassifier(), hyperparameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After being `fit`, the `best_params_` attribute provides us with the best combination of hyperparameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All of the intermediate results ‚Äì validation accuracies for each fold, mean validation accuaries, etc. ‚Äì are stored in the `cv_results_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searcher.cv_results_['mean_test_score'] # Array of length 140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows correspond to folds, columns correspond to hyperparameter combinations.\n",
    "pd.DataFrame(np.vstack([searcher.cv_results_[f'split{i}_test_score'] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above DataFrame tells us that 5 * 140 = 700 models were trained in total!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've found the best combination of hyperparameters, we should fit a decision tree instance using those hyperparameters on our entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree = DecisionTreeClassifier(**searcher.best_params_)\n",
    "final_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy.\n",
    "final_tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy.\n",
    "final_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember, `searcher` itself is a model object (we had to `fit` it). After performing $k$-fold cross-validation, behind the scenes, `searcher` is trained on the entire training set using the optimal combination of hyperparameters.\n",
    "\n",
    "In other words, `searcher` makes the same predictions that `final_tree` does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing possible hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A full grid search can take a **long time**.\n",
    "    - In our previous example, we tried 140 combinations of hyperparameters.\n",
    "    - Since we performed 5-fold cross-validation, we trained 700 decision trees under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we pick the possible hyperparameter values to try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer**: Trial and error.\n",
    "    - If the \"best\" choice of a hyperparameter was at an extreme, try increasing the range.\n",
    "    - For instance, if you try `max_depth`s from 32 to 128, and 32 was the best, try including `max_depths` under 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key takeaways\n",
    "\n",
    "- Decision trees are trained by finding the best questions to ask using the features in the training data. A good question is one that isolates classes as much as possible.\n",
    "- Decision trees have a tendency to overfit to training data. One way to mitigate this is by restricting the maximum depth of the tree.\n",
    "- To efficiently find hyperparameters through cross-validation, use `GridSearchCV`.\n",
    "    - Specify which values to try for each hyperparameter, and `GridSearchCV` will try all **unique combinations of hyperparameters** and return the combination with the best average validation performance.\n",
    "    - `GridSearchCV` is not the only solution ‚Äì see [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heights and weights\n",
    "\n",
    "We have a dataset containing the weights and heights of 25,0000 18 year olds, taken from [here](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.read_csv('data/SOCR-HeightWeight.csv').drop('Index', axis=1)\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people.plot(kind='scatter', x='Height (Inches)', y='Weight (Pounds)', \n",
    "            title='Weight vs. Height for 25,000 18 Year Olds', template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivating example\n",
    "\n",
    "Suppose we fit a simple linear regression model that uses **height in inches** to predict **weight in pounds**.\n",
    "\n",
    "$$\\text{predicted weight (pounds)} = w_0 + w_1 \\cdot \\text{height (inches)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(people[['Height (Inches)']], \n",
    "                                                            people['Weight (Pounds)'], \n",
    "                                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_one_feat = LinearRegression()\n",
    "lr_one_feat.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$w_0^*$ and $w_1^*$ are shown below, along with the model's **testing** RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_one_feat.intercept_, lr_one_feat.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_one_feat = mean_squared_error(y_test_1, \n",
    "                                   lr_one_feat.predict(X_test_1), \n",
    "                                   squared=False)\n",
    "rmse_one_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, suppose we fit another regression model, that uses **height in inches** AND **height in centimeters** to predict weight.\n",
    "\n",
    "$$\\text{predicted weight (pounds)} = w_0 + w_1 \\cdot \\text{height (inches)} + w_2 \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people['Height (cm)'] = people['Height (Inches)'] * 2.54 # 1 inch = 2.54 cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(people[['Height (Inches)', 'Height (cm)']], \n",
    "                                                            people['Weight (Pounds)'], \n",
    "                                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_two_feat = LinearRegression()\n",
    "lr_two_feat.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are $w_0^*$, $w_1^*$, $w_2^*$, and the model's testing RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_two_feat.intercept_, lr_two_feat.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_two_feat = mean_squared_error(y_test_2, \n",
    "                                   lr_two_feat.predict(X_test_2), \n",
    "                                   squared=False)\n",
    "rmse_two_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Observation**: The intercept is the same as before (roughly -81.17), as is the testing RMSE. However, the coefficients on `'Height (Inches)'` and `'Height (cm)'` are massive in size!\n",
    "\n",
    "What's going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Redundant features\n",
    "\n",
    "Let's use simpler numbers for illustration. Suppose in the first model, $w_0^* = -80$ and $w_1^* = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{predicted weight (pounds)} = -80 + 3 \\cdot \\text{height (inches)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the second model, we have:\n",
    "\n",
    "$$\\begin{align*}\\text{predicted weight (pounds)} &= w_0^* + w_1^* \\cdot \\text{height (inches)} + w_2^* \\cdot \\text{height (cm)} \\\\ &= w_0^* + w_1^* \\cdot \\text{height (inches)} + w_2^* \\cdot \\big( 2.54^* \\cdot \\text{height (inches)} \\big) \\\\ &= w_0^* + \\left(w_1^* + 2.54 \\cdot w_2^* \\right) \\cdot \\text{height (inches)} \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the first model, we already found the \"best\" intercept ($-80$) and slope ($3$) in a linear model that uses height in inches to predict weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**So, as long as $w_1^* + 2.54 \\cdot w_2^* = 3$ in the second model, the second model's training predictions will be the same as the first, and hence they will also minimize RMSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Infinitely many parameter choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Issue**: There are an infinite number of $w_1^*$ and $w_2^*$ that satisfy $w_1^* + 2.54 \\cdot w_2^* = 3$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{predicted weight} = -80 - 10 \\cdot \\text{height (inches)} + \\frac{13}{2.54} \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{predicted weight} = -80 + 10 \\cdot \\text{height (inches)} - \\frac{7}{2.54} \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both prediction rules look very different, but actually make the same predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `lr.coef_` could return either set of coefficients, or any other of the infinitely many options. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But neither set of coefficients is **has any meaning!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-80 - 10 * people.iloc[:, 0] + (13 / 2.54) * people.iloc[:, 2]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(-80 + 10 * people.iloc[:, 0] - (7 / 2.54) * people.iloc[:, 2]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multicollinearity occurs when features in a regression model are **highly correlated** with one another.\n",
    "    - In other words, multicollinearity occurs when **a feature can be predicted using a linear combination of other features, fairly accurately**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When multicollinearity is present in the features, the **coefficients in the model** are uninterpretable ‚Äì they have no meaning.\n",
    "    - A \"slope\" represents \"the rate of change of $y$ with respect to a feature\", when all other features are held constant ‚Äì but if there's multicollinearity, you can't hold other features constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Note: Multicollinearity doesn't impact a model's predictions!**\n",
    "    - It doesn't impact a model's ability to generalize to unseen data.\n",
    "    - If features are multicollinear in the training data, they will probably be multicollinear in the test data too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Solutions**:\n",
    "    - Manually remove highly correlated features.\n",
    "    - Use a dimensionality reduction technique (such as PCA) to automatically reduce dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key takeaways\n",
    "\n",
    "- Multicollinearity is present in a linear model when one feature can be accurately predicted using one or more other features.\n",
    "    - In other words, it is present when a feature is **redundant**.\n",
    "- Multicollinearity doesn't pose an issue for prediction; it doesn't hinder a model's ability to generalize. Instead, it renders the **coefficients** of a linear model meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "See the individual sections for more specific \"key takeaways\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time\n",
    "- Multicollinearity and one hot encoding.\n",
    "- Using text features in a predictive model.\n",
    "- Metrics for measuring the performance of classifiers other than accuracy."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
