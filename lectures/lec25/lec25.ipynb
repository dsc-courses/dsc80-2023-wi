{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bd6be",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "TEMPLATE = 'seaborn'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e317133",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 25 â€“ Grid Search, Multicollinearity, Examples\n",
    "\n",
    "## DSC 80, Winter 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4e40bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    "- Lab 9 (pipelines) is due **today at 11:59PM**.\n",
    "- Project 5 (prediction) is due on Thursday, March 23rd at 11:59PM (no slip days allowed)!\n",
    "- There is no live lecture on Wednesday or Friday; videos for these lectures have already been posted on the course website.\n",
    "    - There _is_ still discussion on Wednesday.\n",
    "- The Final Exam is on **Wednesday, March 22nd from 11:30AM-2:30PM**, location TBD.\n",
    "    - Lectures 1-26 (everything before this Friday) are in scope, as are all assignments.\n",
    "    - You can bring 2 two-sided notes sheets. More details to come on Ed.\n",
    "    - [practice.dsc80.com](https://practice.dsc80.com) now contains 3 past finals. Start reviewing!\n",
    "- If at least 80% of the class fills out **BOTH** [CAPEs](https://cape.ucsd.edu) and the [End-of-Quarter Survey](https://docs.google.com/forms/d/e/1FAIpQLSffA3AK7HDGq5HX5hENTKUPE-Z_8W9CXR-eTOp5yT39qd8A9A/viewform), then everyone will receive an extra 0.5% added to their overall course grade.\n",
    "    - Deadline: **Saturday, March 18th at 8AM**.\n",
    "\n",
    "- Look at [this Ed post](https://edstem.org/us/courses/32057/discussion/2768418) for a description of the difference between parameters and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea175d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "- Recap: Decision trees ðŸŒ² and grid search.\n",
    "- Multicollinearity (including how it arises in one hot encoding).\n",
    "- Example: Modeling with text features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb99094",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Decision trees ðŸŒ² and grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8099ac87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Predicting diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad7b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = pd.read_csv('data/diabetes.csv')\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481163a3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig = (\n",
    "    diabetes.assign(Outcome=diabetes['Outcome'].astype(str))\n",
    "            .plot(kind='scatter', x='Glucose', y='BMI', color='Outcome', \n",
    "                  color_discrete_map={'0': 'orange', '1': 'blue'},\n",
    "                  title='Relationship between Glucose, BMI, and Diabetes',\n",
    "                  template=TEMPLATE)\n",
    ")\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5365f9cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recall, we started with a relatively simple decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6398aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes[['Glucose', 'BMI']], \n",
    "                                                    diabetes['Outcome'],\n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f20695",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=2)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dd2e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "plot_tree(dt, feature_names=X_train.columns, class_names=['no db', 'yes db'], \n",
    "          filled=True, rounded=True, fontsize=15, impurity=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71d335",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Goal\n",
    "\n",
    "Create a `DecisionTreeClassifier` that "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104e731",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- will **generalize well** to unseen data, by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2521789",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- finding the **combination of hyperparameters (`max_depth`, `min_samples_split`, `criterion`)** that maximizes average validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b24c3ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Grid search\n",
    "\n",
    "`GridSearchCV` takes in:\n",
    "- an **un-`fit`** instance of an estimator, and\n",
    "- a **dictionary** of hyperparameter values to try,\n",
    "\n",
    "and performs $k$-fold cross-validation to find the **combination of hyperparameters with the best average validation performance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbae62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea030231",
   "metadata": {},
   "source": [
    "The following dictionary contains the values we're considering for each hyperparameter. (We're using `GridSearchCV` with 3 hyperparameters, but we could use it with even just a single hyperparameter.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'max_depth': [2, 3, 4, 5, 7, 10, 13, 15, 18, None], \n",
    "    'min_samples_split': [2, 5, 10, 20, 50, 100, 200],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888fb8f7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that there are 140 **combinations** of hyperparameters we need to try. We need to find the **best combination** of hyperparameters, not the best value for each hyperparameter individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66815420",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hyperparameters['max_depth']) * \\\n",
    "len(hyperparameters['min_samples_split']) * \\\n",
    "len(hyperparameters['criterion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039596f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`GridSearchCV` needs to be instantiated and `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9d779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = GridSearchCV(DecisionTreeClassifier(), hyperparameters, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b9c11",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "After being `fit`, the `best_params_` attribute provides us with the best combination of hyperparameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffdbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7cf11c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All of the intermediate results â€“ validation accuracies for each fold, mean validation accuaries, etc. â€“ are stored in the `cv_results_` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b91f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "searcher.cv_results_['mean_test_score'] # Array of length 140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows correspond to folds, columns correspond to hyperparameter combinations.\n",
    "pd.DataFrame(np.vstack([searcher.cv_results_[f'split{i}_test_score'] for i in range(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccddf2ff",
   "metadata": {},
   "source": [
    "Note that the above DataFrame tells us that 5 * 140 = 700 models were trained in total!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce5954",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've found the best combination of hyperparameters, we should fit a decision tree instance using those hyperparameters on our entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba26fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be463a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree = DecisionTreeClassifier(**searcher.best_params_)\n",
    "final_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy.\n",
    "final_tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0065e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy.\n",
    "final_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08638ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember, `searcher` itself is a model object (we had to `fit` it). After performing $k$-fold cross-validation, behind the scenes, `searcher` is trained on the entire training set using the optimal combination of hyperparameters.\n",
    "\n",
    "In other words, `searcher` makes the same predictions that `final_tree` does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f8831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf0528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing possible hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb97aa5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A full grid search can take a **long time**.\n",
    "    - In our previous example, we tried 140 combinations of hyperparameters.\n",
    "    - Since we performed 5-fold cross-validation, we trained 700 decision trees under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c315034",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Question**: How do we pick the possible hyperparameter values to try?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526a80a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Answer**: Trial and error.\n",
    "    - If the \"best\" choice of a hyperparameter was at an extreme, try increasing the range.\n",
    "    - For instance, if you try `max_depth`s from 32 to 128, and 32 was the best, try including `max_depth`s under 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da9e3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key takeaways\n",
    "\n",
    "- Decision trees are trained by finding the best questions to ask using the features in the training data. A good question is one that isolates classes as much as possible.\n",
    "- Decision trees have a tendency to overfit to training data. One way to mitigate this is by restricting the maximum depth of the tree.\n",
    "- To efficiently find hyperparameters through cross-validation, use `GridSearchCV`.\n",
    "    - Specify which values to try for each hyperparameter, and `GridSearchCV` will try all **unique combinations of hyperparameters** and return the combination with the best average validation performance.\n",
    "    - `GridSearchCV` is not the only solution â€“ see [`RandomizedSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) if you're curious."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e7c89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27119ea6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Heights and weights\n",
    "\n",
    "We have a dataset containing the weights and heights of 25,0000 18 year olds, taken from [here](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fc7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.read_csv('data/SOCR-HeightWeight.csv').drop('Index', axis=1)\n",
    "people.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab54d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "people.plot(kind='scatter', x='Height (Inches)', y='Weight (Pounds)', \n",
    "            title='Weight vs. Height for 25,000 18 Year Olds', template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c45e5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivating example\n",
    "\n",
    "Suppose we fit a simple linear regression model that uses **height in inches** to predict **weight in pounds**.\n",
    "\n",
    "$$\\text{predicted weight (pounds)} = w_0 + w_1 \\cdot \\text{height (inches)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36df7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b06d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(people[['Height (Inches)']], \n",
    "                                                            people['Weight (Pounds)'], \n",
    "                                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df01fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_one_feat = LinearRegression()\n",
    "lr_one_feat.fit(X_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363d4c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$w_0^*$ and $w_1^*$ are shown below, along with the model's **testing** RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b214760",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_one_feat.intercept_, lr_one_feat.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a32abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_one_feat = mean_squared_error(y_test_1, \n",
    "                                   lr_one_feat.predict(X_test_1), \n",
    "                                   squared=False)\n",
    "rmse_one_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d01d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, suppose we fit another regression model, that uses **height in inches** AND **height in centimeters** to predict weight.\n",
    "\n",
    "$$\\text{predicted weight (pounds)} = w_0 + w_1 \\cdot \\text{height (inches)} + w_2 \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "people['Height (cm)'] = people['Height (Inches)'] * 2.54 # 1 inch = 2.54 cm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e87ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(people[['Height (Inches)', 'Height (cm)']], \n",
    "                                                            people['Weight (Pounds)'], \n",
    "                                                            random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d693a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_two_feat = LinearRegression()\n",
    "lr_two_feat.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f73714",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are $w_0^*$, $w_1^*$, $w_2^*$, and the model's testing RMSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_two_feat.intercept_, lr_two_feat.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b307b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_two_feat = mean_squared_error(y_test_2, \n",
    "                                   lr_two_feat.predict(X_test_2), \n",
    "                                   squared=False)\n",
    "rmse_two_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0592266",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Observation**: The intercept is the same as before (roughly -81.17), as is the testing RMSE. However, the coefficients on `'Height (Inches)'` and `'Height (cm)'` are massive in size!\n",
    "\n",
    "What's going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc1c3fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Redundant features\n",
    "\n",
    "Let's use simpler numbers for illustration. Suppose in the first model, $w_0^* = -80$ and $w_1^* = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b8e23a",
   "metadata": {},
   "source": [
    "$$\\text{predicted weight (pounds)} = -80 + 3 \\cdot \\text{height (inches)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc047bf6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the second model, we have:\n",
    "\n",
    "$$\\begin{align*}\\text{predicted weight (pounds)} &= w_0^* + w_1^* \\cdot \\text{height (inches)} + w_2^* \\cdot \\text{height (cm)} \\\\ &= w_0^* + w_1^* \\cdot \\text{height (inches)} + w_2^* \\cdot \\big( 2.54^* \\cdot \\text{height (inches)} \\big) \\\\ &= w_0^* + \\left(w_1^* + 2.54 \\cdot w_2^* \\right) \\cdot \\text{height (inches)} \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0878dcc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the first model, we already found the \"best\" intercept ($-80$) and slope ($3$) in a linear model that uses height in inches to predict weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c4aee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**So, as long as $w_1^* + 2.54 \\cdot w_2^* = 3$ in the second model, the second model's training predictions will be the same as the first, and hence they will also minimize RMSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cb0db0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Infinitely many parameter choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7f7e7",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Issue**: There are an infinite number of $w_1^*$ and $w_2^*$ values that satisfy $w_1^* + 2.54 \\cdot w_2^* = 3$!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ba11e4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{predicted weight} = -80 - 10 \\cdot \\text{height (inches)} + \\frac{13}{2.54} \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b9d16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{predicted weight} = -80 + 10 \\cdot \\text{height (inches)} - \\frac{7}{2.54} \\cdot \\text{height (cm)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a6e498",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Both prediction rules look very different, but actually make the same predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffabff3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `lr.coef_` could return either set of coefficients, or any other of the infinitely many options. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0fb16",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- But neither set of coefficients **has any meaning!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "(-80 - 10 * people.iloc[:, 0] + (13 / 2.54) * people.iloc[:, 2]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769bf4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "(-80 + 10 * people.iloc[:, 0] - (7 / 2.54) * people.iloc[:, 2]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05303752",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070195b5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multicollinearity occurs when features in a regression model are **highly correlated** with one another.\n",
    "    - In other words, multicollinearity occurs when **a feature can be predicted using a linear combination of other features, fairly accurately**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff0cfa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- When multicollinearity is present in the features, the **coefficients in the model** are uninterpretable â€“ they have no meaning.\n",
    "    - A \"slope\" represents \"the rate of change of $y$ with respect to a feature\", when all other features are held constant â€“ but if there's multicollinearity, you can't hold other features constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a80205",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Note: Multicollinearity doesn't impact a model's predictions!**\n",
    "    - It doesn't impact a model's ability to generalize to unseen data.\n",
    "    - If features are multicollinear in the training data, they will probably be multicollinear in the test data too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aed7913",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Solutions**:\n",
    "    - Manually remove highly correlated features.\n",
    "    - Use a dimensionality reduction technique (such as PCA) to automatically reduce dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de8984",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One hot encoding and multicollinearity\n",
    "\n",
    "When we one hot encode categorical features, we create several **redundant** columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef5184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tips = px.data.tips()\n",
    "tips_features = tips.drop('tip', axis=1)\n",
    "tips_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4544a2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Aside: You can use `pd.get_dummies` in EDA, but **don't** use it for modeling (instead, use `OneHotEncoder`, which works with `Pipeline`s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af467144",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(tips_features)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c217b6ec",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember that under the hood, `LinearRegression()` creates a **design matrix** that has a column of all ones (for the intercept term). Let's add that column above for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800933cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['all_ones'] = 1\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7917a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, many of the above columns **can be written as linear combinations of other columns**!\n",
    "- We don't need `'sex_Male'` â€“ its value is just `'all_ones'` - `'sex_Female'`.\n",
    "- We don't need `'smoker_Yes'` â€“ its value is just `'all_ones'` - `'smoker_No'`.\n",
    "- We don't need `'time_Lunch'` â€“ its value is just `'all_ones'` - `'time_Dinner'`.\n",
    "- We don't need `'day_Thur'` â€“ its value is just `'all_ones'` - (`'day_Fri'` + `'day_Sat'` + `'day_Sun'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f2265",
   "metadata": {},
   "source": [
    "Note that if we get rid of the four redundant columns above, the **rank** of our design matrix â€“ that is, the number of linearly independent columns it has â€“ does not change (and so the \"predictive power\" of our features don't change either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9689c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(X.drop(columns=['sex_Male', 'smoker_Yes', 'time_Lunch', 'day_Thur']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b2930",
   "metadata": {},
   "source": [
    "However, without the redundant columns, there is only a single unique set of optimal parameters $w^*$, and the multicollinearity is no more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d305f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Aside**: Most one hot encoding techniques (including `OneHotEncoder`) have an in-built `drop` argument, which allow you to specify that you'd like to drop **one column per categorical feature**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcdbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(tips_features, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2c75a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop='first')\n",
    "ohe.fit_transform(tips_features[['sex', 'smoker', 'day', 'time']]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e63fa",
   "metadata": {},
   "source": [
    "The above array only has $(2-1) + (2-1) + (4-1) + (2-1) = 6$ columns, rather than $2 + 2 + 4 + 2 = 10$, since we dropped 1 per categorical column in `tips_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e79a4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key takeaways\n",
    "\n",
    "- Multicollinearity is present in a linear model when one feature can be accurately predicted using one or more other features.\n",
    "    - In other words, it is present when a feature is **redundant**.\n",
    "- Multicollinearity doesn't pose an issue for prediction; it doesn't hinder a model's ability to generalize. Instead, it renders the **coefficients** of a linear model meaningless.\n",
    "- Multicollinearity is present when performing one hot encoding; a solution is to **drop one one hot encoded column for each original categorical feature**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b490f5b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Modeling using text features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa4babc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Predicting reviews\n",
    "\n",
    "We have a dataset containing Amazon reviews and ratings for patio, lawn, and gardening products. (Aside: [Here](https://cseweb.ucsd.edu/~jmcauley/datasets.html) is a good source for such data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa87d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_json(open('data/reviews.json'), lines=True)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d2c890",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Goal**: Use a review's `'summary'` to predict its `'overall'` rating.\n",
    "\n",
    "Note that there are five possible `'overall'` rating values â€“ 1, 2, 3, 4, 5 â€“ not just two. As such, this is an instance of **multiclass classification**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd86df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['overall'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e5b7c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Question**: What is the worst possible accuracy we should expect from a ratings classifier, given the above distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450ceb13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aside: `CountVectorizer`\n",
    "\n",
    "Entries in the `'summary'` column are not currently quantitative! We can use the bag of words encoding to create quantitative features out of each `'summary'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78847cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " Instead of performing a bag of words encoding manually as we did before, we can rely on `sklearn`'s `CountVectorizer`. (There is also a `TfidfVectorizer`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e2abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corp = ['hey hey hey my name is billy', \n",
    "                'hey billy how is your dog billy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621045d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer()\n",
    "count_vec.fit(example_corp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c51d02",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`count_vec` learned a **vocabulary** from the corpus we `fit` it on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ea128",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b139d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec.transform(example_corp).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1289714",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that the values in `count_vec.vocabulary_` correspond to the positions of the columns in `count_vec.transform(example_corp).toarray()`, i.e. `'billy'` is the first column and `'your'` is the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2725f12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(count_vec.transform(example_corp).toarray(),\n",
    "             columns=pd.Series(count_vec.vocabulary_).sort_values().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf092f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating an initial `Pipeline`\n",
    "\n",
    "Let's build a `Pipeline` that takes in summaries and overall ratings and:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b489b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Uses `CountVectorizer` to quantitatively encode summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22c18a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Fits a `RandomForestClassifier` to the data.\n",
    "    - A \"random forest\" is a combination (or **ensemble**) of decision trees, each fit on a different **bootstrapped** resample of the training data.\n",
    "    - It makes predictions by aggregating the results of the individual trees (in the case of classification, by taking the **most common prediction**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87815af4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But first, a train-test split (like **always**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f441fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79e2bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews['summary']\n",
    "y = reviews['overall']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc4fd32",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To start, we'll create a random forest with 7 trees (`n_estimators`) each of which has a maximum depth of 8 (`max_depth`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ca601",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Pipeline([\n",
    "    ('cv', CountVectorizer()), \n",
    "    ('clf', RandomForestClassifier(max_depth=8, n_estimators=7)) # Uses 7 separate decision trees!\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d085438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy.\n",
    "pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ca110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy.\n",
    "pl.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3085c7a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The accuracy of our random forest is just above 50%, on both the training and testing sets. We'd get the same performance by predicting a rating of 5 every time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546bee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of true ys in the training set: 53% are 5.\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce2817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of predicted ys in the training set: 99.8% are 5.\n",
    "# It turns out we essentially are predicting 5 every time!\n",
    "pd.Series(pl.predict(X_train)).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bf6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pl.named_steps['cv'].vocabulary_) # We have many features, but we are not asking many questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfd0cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Choosing tree depth via `GridSearchCV`\n",
    "\n",
    "We arbitrarily chose `max_depth=8` before, but it seems like that isn't working well. Let's perform a grid search to find the `max_depth` with the best generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we've used the key clf__max_depth, not max_depth\n",
    "# because max_depth is a hyperparameter of clf, not of pl.\n",
    "\n",
    "hyperparameters = {\n",
    "    'clf__max_depth': np.arange(2, 500, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca526a3",
   "metadata": {},
   "source": [
    "Note that while `pl` has already been `fit`, we can still give it to `GridSearchCV`, which will repeatedly re-`fit` it during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c8a58f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Takes 10+ seconds to run â€“ how many trees are being trained?\n",
    "grids = GridSearchCV(pl, param_grid=hyperparameters, return_train_score=True)\n",
    "grids.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9827c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "grids.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e9e9d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall, `fit` `GridSearchCV` objects are estimators on their own as well. This means we can compute the training and testing accuracies of the \"best\" random forest directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c75868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy.\n",
    "grids.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cb1305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy.\n",
    "grids.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745c864",
   "metadata": {},
   "source": [
    "Still not much better on the testing set! ðŸ¤·"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f916750",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training and validation accuracy vs. depth\n",
    "\n",
    "Below, we plot how training and validation accuracy varied with tree depth. Note that the $y$-axis here is accuracy, and that larger accuracies are better (unlike with RMSE, where smaller was better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef11897",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = grids.param_grid['clf__max_depth']\n",
    "train = grids.cv_results_['mean_train_score']\n",
    "valid = grids.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370ae59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'train': train, 'valid': valid}, index=index).plot().update_layout(\n",
    "    xaxis_title='max_depth', yaxis_title='Accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d45f6",
   "metadata": {},
   "source": [
    "Unsurprisingly, training accuracy kept increasing, while validation accuracy leveled off around a depth of ~100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebcd708",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary, next time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f4fbe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "- See the grid search and multicollinearity sections for more specific \"key takeaways\".\n",
    "- The `CountVectorizer` transformer can be used to perform the bag of words encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa34fd34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Next time\n",
    "\n",
    "Metrics for measuring the performance of classifiers other than accuracy."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
